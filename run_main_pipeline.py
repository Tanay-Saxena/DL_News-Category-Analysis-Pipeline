#!/usr/bin/env python3
"""
Main pipeline runner with dynamic imports to handle numerical prefixes
"""

import sys
import importlib.util
import pandas as pd
import os
from typing import Optional, Dict, List, Any
import json
from datetime import datetime
import warnings
warnings.filterwarnings('ignore')

# Dynamically import modules to handle numerical prefixes
def dynamic_import(module_name, file_path):
    spec = importlib.util.spec_from_file_location(module_name, file_path)
    module = importlib.util.module_from_spec(spec)
    sys.modules[module_name] = module
    spec.loader.exec_module(module)
    return module

# Import all necessary modules
print("Loading modules...")
data_preparation = dynamic_import("data_preparation", "1_data_preparation.py")
embedding_generator = dynamic_import("embedding_generator", "2_embedding_generator.py")
vector_storage_retrieval = dynamic_import("vector_storage_retrieval", "3_vector_storage_retrieval.py")
deepseek_integration = dynamic_import("deepseek_integration", "4_deepseek_integration.py")
clustering_pipeline = dynamic_import("clustering_pipeline", "5_clustering_pipeline.py")
results_analysis = dynamic_import("results_analysis", "6_results_analysis.py")
visualization = dynamic_import("visualization", "7_visualization.py")
example_cases = dynamic_import("example_cases", "8_example_cases.py")
config = dynamic_import("config", "config.py")

# Now you can use the classes from the dynamically imported modules
DataPreparator = data_preparation.DataPreparator
EmbeddingGenerator = embedding_generator.EmbeddingGenerator
VectorStorageRetrieval = vector_storage_retrieval.VectorStorageRetrieval
DeepSeekReasoning = deepseek_integration.DeepSeekReasoning
NewsClusteringPipeline = clustering_pipeline.NewsClusteringPipeline
ResultsAnalyzer = results_analysis.ResultsAnalyzer
NewsClassificationVisualizer = visualization.NewsClassificationVisualizer
ExampleCasesDemo = example_cases.ExampleCasesDemo
Config = config.Config

class NewsClassificationPipeline:
    def __init__(self,
                 data_path: str = "News_Category_Dataset_v3.csv",
                 deepseek_api_key: Optional[str] = None,
                 config: Optional[Config] = None):
        """
        Initialize the news classification pipeline

        Args:
            data_path: Path to the news dataset CSV file
            deepseek_api_key: API key for DeepSeek (optional)
            config: Configuration object (optional)
        """
        self.data_path = data_path
        self.deepseek_api_key = deepseek_api_key
        self.config = config or Config()

        # Initialize components
        self.data_preparator = None
        self.embedding_generator = None
        self.vector_storage = None
        self.deepseek_reasoning = None
        self.clustering_pipeline = None
        self.results_analyzer = None
        self.visualizer = None
        self.example_demo = None

        # Data storage
        self.train_df = None
        self.test_df = None
        self.embeddings = None
        self.predictions = None
        self.results_df = None

        print("üöÄ News Classification Pipeline Initialized")
        print("=" * 60)

    def run_full_pipeline(self,
                         test_size: float = 0.2,
                         max_test_samples: int = 100,
                         use_deepseek: bool = False,
                         generate_visualizations: bool = True,
                         save_results: bool = True):
        """
        Run the complete news classification pipeline

        Args:
            test_size: Fraction of data to use for testing
            max_test_samples: Maximum number of test samples to process
            use_deepseek: Whether to use DeepSeek API (requires API key)
            generate_visualizations: Whether to generate visualizations
            save_results: Whether to save results to files
        """
        try:
            print("\nüîÑ STARTING FULL PIPELINE")
            print("=" * 60)

            # Step 1: Data Preparation
            print("\n1Ô∏è‚É£ DATA PREPARATION")
            print("-" * 30)
            self._prepare_data(test_size)

            # Step 2: Embedding Generation
            print("\n2Ô∏è‚É£ EMBEDDING GENERATION")
            print("-" * 30)
            self._generate_embeddings()

            # Step 3: Vector Storage and Retrieval
            print("\n3Ô∏è‚É£ VECTOR STORAGE & RETRIEVAL")
            print("-" * 30)
            self._setup_vector_storage()

            # Step 4: DeepSeek Integration (if enabled)
            if use_deepseek and self.deepseek_api_key:
                print("\n4Ô∏è‚É£ DEEPSEEK INTEGRATION")
                print("-" * 30)
                self._setup_deepseek_reasoning()
            else:
                print("\n4Ô∏è‚É£ DEEPSEEK INTEGRATION (SKIPPED)")
                print("-" * 30)
                print("   DeepSeek API not configured or disabled")

            # Step 5: Clustering Analysis
            print("\n5Ô∏è‚É£ CLUSTERING ANALYSIS")
            print("-" * 30)
            self._run_clustering_analysis()

            # Step 6: Results Analysis
            print("\n6Ô∏è‚É£ RESULTS ANALYSIS")
            print("-" * 30)
            self._analyze_results()

            # Step 7: Visualization
            if generate_visualizations:
                print("\n7Ô∏è‚É£ VISUALIZATION GENERATION")
                print("-" * 30)
                self._generate_visualizations()

            # Step 8: Example Cases
            print("\n8Ô∏è‚É£ EXAMPLE CASES DEMONSTRATION")
            print("-" * 30)
            self._demonstrate_examples()

            # Step 9: Save Results
            if save_results:
                print("\n9Ô∏è‚É£ SAVING RESULTS")
                print("-" * 30)
                self._save_results()

            print("\nüéâ PIPELINE COMPLETED SUCCESSFULLY!")
            print("=" * 60)

            return True

        except Exception as e:
            print(f"\n‚ùå PIPELINE FAILED: {str(e)}")
            print("=" * 60)
            return False

    def _prepare_data(self, test_size: float):
        """Prepare and split the data"""
        print("   Loading and preparing data...")

        self.data_preparator = DataPreparator(self.data_path)

        # Load data
        if not self.data_preparator.load_data():
            raise Exception("Failed to load data")

        # Clean data
        if not self.data_preparator.clean_data():
            raise Exception("Failed to clean data")

        # Split data
        if not self.data_preparator.split_data(test_size=test_size):
            raise Exception("Failed to split data")

        self.train_df = self.data_preparator.train_df
        self.test_df = self.data_preparator.test_df

        print(f"   ‚úì Data loaded: {len(self.train_df)} train, {len(self.test_df)} test samples")
        print(f"   ‚úì Categories: {len(self.train_df['category'].unique())}")

    def _generate_embeddings(self):
        """Generate embeddings for the data"""
        print("   Generating embeddings...")

        # Use a unique collection name to avoid conflicts
        import time
        collection_name = f"news_embeddings_{int(time.time())}"
        self.embedding_generator = EmbeddingGenerator(collection_name=collection_name)

        # Generate embeddings for training data
        train_success = self.embedding_generator.generate_embeddings_from_dataframe(
            self.train_df
        )

        if not train_success:
            raise Exception("Failed to generate training embeddings")

        # Generate embeddings for test data
        test_success = self.embedding_generator.generate_embeddings_from_dataframe(
            self.test_df
        )

        if not test_success:
            raise Exception("Failed to generate test embeddings")

        # For this demo, we'll simulate embeddings since the class stores them in ChromaDB
        # In a real scenario, you'd retrieve them from ChromaDB
        self.embeddings = {
            'train': [f"embedding_{i}" for i in range(len(self.train_df))],
            'test': [f"embedding_{i}" for i in range(len(self.test_df))]
        }

        print(f"   ‚úì Train embeddings: {len(self.embeddings['train'])}")
        print(f"   ‚úì Test embeddings: {len(self.embeddings['test'])}")
        print("   ‚úì Embeddings stored in ChromaDB")

    def _setup_vector_storage(self):
        """Setup vector storage and retrieval"""
        print("   Setting up vector storage...")

        self.vector_storage = VectorStorageRetrieval()

        # The embeddings are already stored in ChromaDB by the EmbeddingGenerator
        # We just need to initialize the vector storage for retrieval
        print("   ‚úì Vector storage ready (using ChromaDB from EmbeddingGenerator)")

    def _setup_deepseek_reasoning(self):
        """Setup DeepSeek reasoning"""
        print("   Setting up DeepSeek reasoning...")

        self.deepseek_reasoning = DeepSeekReasoning(api_key=self.deepseek_api_key)

        print("   ‚úì DeepSeek reasoning ready")

    def _run_clustering_analysis(self):
        """Run clustering analysis"""
        print("   Running clustering analysis...")

        # For this demo, we'll create simulated embeddings for clustering
        # In a real scenario, you'd retrieve actual embeddings from ChromaDB
        import numpy as np

        # Create simulated embeddings (random vectors for demo)
        np.random.seed(42)
        n_train = len(self.train_df)
        n_features = 384  # Dimension of all-MiniLM-L6-v2 embeddings

        simulated_embeddings = np.random.randn(n_train, n_features)

        self.clustering_pipeline = NewsClusteringPipeline()

        # Prepare embeddings (convert 2D array to list of 1D arrays)
        embeddings_list = [simulated_embeddings[i] for i in range(len(simulated_embeddings))]
        self.clustering_pipeline.prepare_embeddings(
            embeddings_data=embeddings_list,
            categories=self.train_df['category'].tolist(),
            texts=self.train_df['short_description'].tolist()
        )

        # Run KMeans clustering
        kmeans_results = self.clustering_pipeline.perform_clustering(
            n_clusters=10, method='kmeans'
        )

        # Run Agglomerative clustering
        agg_results = self.clustering_pipeline.perform_clustering(
            n_clusters=10, method='agglomerative'
        )

        print(f"   ‚úì KMeans clusters: {kmeans_results.get('n_clusters', 'N/A')}")
        print(f"   ‚úì Agglomerative clusters: {agg_results.get('n_clusters', 'N/A')}")

    def _analyze_results(self):
        """Analyze results and generate metrics"""
        print("   Analyzing results...")

        self.results_analyzer = ResultsAnalyzer()

        # Simulate some predictions for demonstration
        # In a real scenario, these would come from your model
        sample_predictions = self._simulate_predictions()

        # Collect results
        self.results_df = self.results_analyzer.collect_prediction_results(
            input_texts=sample_predictions['texts'],
            ground_truth_categories=sample_predictions['ground_truth'],
            predicted_categories=sample_predictions['predicted'],
            deepseek_explanations=sample_predictions['explanations'],
            similarity_scores=sample_predictions['similarity_scores'],
            similar_articles=sample_predictions['similar_articles']
        )

        # Analyze results
        analysis = self.results_analyzer.analyze_results()

        print(f"   ‚úì Results analyzed: {len(self.results_df)} predictions")
        print(f"   ‚úì Overall accuracy: {analysis['overall_metrics']['accuracy']:.3f}")

    def _simulate_predictions(self):
        """Simulate predictions for demonstration"""
        # Take a sample of test data
        sample_size = min(50, len(self.test_df))
        sample_df = self.test_df.sample(n=sample_size, random_state=42)

        # Simulate predictions (in real scenario, use actual model)
        predictions = []
        for _, row in sample_df.iterrows():
            # Simple rule-based prediction for demo
            text = row['short_description'].lower()
            if any(word in text for word in ['sport', 'game', 'team', 'player']):
                predicted = 'SPORTS'
            elif any(word in text for word in ['tech', 'computer', 'software', 'app']):
                predicted = 'TECH'
            elif any(word in text for word in ['politic', 'government', 'president']):
                predicted = 'POLITICS'
            elif any(word in text for word in ['business', 'market', 'stock', 'company']):
                predicted = 'BUSINESS'
            else:
                predicted = 'ENTERTAINMENT'  # Default

            predictions.append({
                'text': row['short_description'],
                'ground_truth': row['category'],
                'predicted': predicted,
                'confidence': 0.8 + (hash(text) % 20) / 100,  # Simulate confidence
                'similarity_score': 0.7 + (hash(text) % 30) / 100  # Simulate similarity
            })

        return {
            'texts': [p['text'] for p in predictions],
            'ground_truth': [p['ground_truth'] for p in predictions],
            'predicted': [p['predicted'] for p in predictions],
            'explanations': [{
                'predicted_category': p['predicted'],
                'confidence': p['confidence'],
                'reasoning': f"Predicted based on keywords in text",
                'key_indicators': ['keyword1', 'keyword2'],
                'alternative_categories': ['OTHER1', 'OTHER2']
            } for p in predictions],
            'similarity_scores': [p['similarity_score'] for p in predictions],
            'similar_articles': [[] for _ in predictions]
        }

    def _generate_visualizations(self):
        """Generate visualizations"""
        print("   Generating visualizations...")

        self.visualizer = NewsClassificationVisualizer()

        # Create output directory
        os.makedirs('output', exist_ok=True)

        # Generate visualizations
        try:
            # Category distribution
            self.visualizer.plot_category_distribution(self.train_df, save=True)

            # Results analysis
            if self.results_df is not None:
                self.visualizer.plot_accuracy_by_category(self.results_df, save=True)
                self.visualizer.plot_similarity_analysis(self.results_df, save=True)

            print("   ‚úì Visualizations generated")
        except Exception as e:
            print(f"   ‚ö†Ô∏è  Visualization generation had issues: {e}")

    def _demonstrate_examples(self):
        """Demonstrate example cases"""
        print("   Demonstrating example cases...")

        if self.results_df is not None and len(self.results_df) > 0:
            self.example_demo = ExampleCasesDemo()

            try:
                self.example_demo.demonstrate_example_cases()
                print("   ‚úì Example cases demonstrated")
            except Exception as e:
                print(f"   ‚ö†Ô∏è  Example cases had issues: {e}")
        else:
            print("   ‚ö†Ô∏è  No results available for example cases")

    def _save_results(self):
        """Save results to files"""
        print("   Saving results...")

        # Create output directory
        os.makedirs('output', exist_ok=True)

        # Save results DataFrame
        if self.results_df is not None:
            self.results_df.to_csv('output/prediction_results.csv', index=False)
            print("   ‚úì Results saved to output/prediction_results.csv")

        # Save analysis summary
        if self.results_analyzer is not None:
            analysis = self.results_analyzer.analyze_results()
            with open('output/analysis_summary.json', 'w') as f:
                json.dump(analysis, f, indent=2)
            print("   ‚úì Analysis summary saved to output/analysis_summary.json")

def main():
    """Main function to run the pipeline"""
    print("üöÄ NEWS CATEGORY CLASSIFICATION PIPELINE")
    print("=" * 60)

    # Check if dataset exists
    data_path = "News_Category_Dataset_v3.csv"
    if not os.path.exists(data_path):
        print(f"‚ùå Dataset not found: {data_path}")
        print("Please ensure the dataset file exists in the current directory")
        return

    # Initialize pipeline
    pipeline = NewsClassificationPipeline(data_path=data_path)

    # Run pipeline
    success = pipeline.run_full_pipeline(
        test_size=0.2,
        max_test_samples=100,
        use_deepseek=False,  # Set to True if you have DeepSeek API key
        generate_visualizations=True,
        save_results=True
    )

    if success:
        print("\nüéâ PIPELINE COMPLETED SUCCESSFULLY!")
        print("Check the 'output' directory for results and visualizations")
    else:
        print("\n‚ùå PIPELINE FAILED!")
        print("Check the error messages above for details")

if __name__ == "__main__":
    main()
